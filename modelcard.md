# üìò Custom Training Components in PyTorch
### Loss Functions and Optimizers ‚Äì Educational Implementations

---

## üìå Overview

This repository contains custom implementations of fundamental training components in [PyTorch](https://pytorch.org/), including:

- Cross-Entropy Loss
- Focal Loss
- Mean Squared Error (MSE) Loss
- Stochastic Gradient Descent (SGD)
- SGD with Momentum

These implementations were developed for educational purposes to deeply understand:

- Autograd mechanics
- Gradient flow
- Optimizer state management
- Loss function mathematics
- Class imbalance handling
- Training loop construction

---

## üß† Implemented Components

---

### 1Ô∏è‚É£ Custom Cross-Entropy Loss

**Type:** Multi-class classification loss  

#### Mathematical Form:

L = -(1/N) * Œ£ log(p_y)

Where:
- p_y is the predicted probability of the true class.
- `log_softmax` is used for numerical stability.

#### Key Design Decisions:
- Uses `torch.log_softmax` instead of `softmax + log`
- Index-based class selection
- Mean reduction

#### Use Case:
- Standard multi-class classification problems

---

### 2Ô∏è‚É£ Custom Focal Loss

**Type:** Imbalance-aware classification loss  

#### Mathematical Form:

L = - (1 - p_t)^Œ≥ * log(p_t)

Where:
- p_t = probability of true class
- Œ≥ (gamma) = focusing parameter

#### Purpose:
- Down-weights easy examples
- Focuses training on hard samples

#### Key Feature:
- Adjustable `gamma` parameter
- Built using log-softmax for stability

#### Use Case:
- Imbalanced datasets
- Object detection
- Rare-class classification

---

### 3Ô∏è‚É£ Custom MSE Loss

**Type:** Regression loss  

#### Formula:

L = (1/N) * Œ£ (y_pred - y_true)^2

#### Purpose:
- Measures squared error between predictions and targets
- Used in regression tasks

---

## ‚öôÔ∏è Custom Optimizers

---

### 4Ô∏è‚É£ Custom SGD

#### Update Rule:

Œ∏ = Œ∏ - Œ∑ * ‚àáL

Where:
- Œ∑ = learning rate

#### Implementation Highlights:
- Extends `torch.optim.Optimizer`
- Iterates over `param_groups`
- Performs direct gradient-based updates

#### Use Case:
- Baseline optimization
- Understanding gradient descent mechanics

---

### 5Ô∏è‚É£ Custom SGD with Momentum

#### Update Rule:

v_t = Œº * v_(t-1) + ‚àáL  
Œ∏ = Œ∏ - Œ∑ * v_t

Where:
- Œº = momentum coefficient
- v_t = velocity

#### Implementation Highlights:
- Maintains per-parameter state dictionary
- Stores velocity tensor
- Demonstrates optimizer state handling

#### Advantages Over Vanilla SGD:
- Faster convergence
- Reduced oscillation
- Smoother optimization trajectory

---

## üß™ Experimental Validation

The components were tested using:

- Synthetic classification logits
- Small regression datasets
- Simple neural network training loops
- Gradient verification using `.backward()`

Example:
- Linear regression trained with Custom SGD
- Observed decreasing loss over epochs
- Verified gradient propagation

---

## üîç Comparison to Native PyTorch

| Component | Custom Version | PyTorch Equivalent |
|------------|----------------|--------------------|
| Cross Entropy | `CustomCrossEntropyLoss` | `nn.CrossEntropyLoss` |
| Focal Loss | `FocalLoss` | Not built-in (custom required) |
| MSE | `CustomMSELoss` | `nn.MSELoss` |
| SGD | `CustomSGD` | `torch.optim.SGD` |
| SGD + Momentum | `CustomSGDMomentum` | `torch.optim.SGD(momentum=...)` |

---

## üéØ Design Philosophy

These implementations prioritize:

- Educational clarity over production efficiency
- Explicit tensor operations
- Understanding gradient computation
- Demonstrating how optimizers maintain internal state

---

## ‚ö†Ô∏è Limitations

- No support for:
  - Weight decay
  - Nesterov momentum
  - Mixed precision training
  - Distributed training
- No label smoothing in cross-entropy
- No alpha-balancing in focal loss
- Minimal error handling

These are simplified implementations intended for learning.

---

## üìä When to Use Each Component

| Scenario | Recommended Component |
|----------|-----------------------|
| Multi-class classification | Custom Cross Entropy |
| Imbalanced classification | Focal Loss |
| Regression | Custom MSE |
| Simple baseline training | Custom SGD |
| Faster convergence | SGD with Momentum |

---

## üéì Educational Value

This project demonstrates understanding of:

- Autograd mechanics
- Loss derivation
- Numerical stability practices
- Optimizer state management
- Parameter group iteration
- Training loop construction

---

## ‚öñÔ∏è Ethical Considerations

- Focal Loss may amplify bias if minority classes are poorly represented.
- Improper hyperparameter tuning can destabilize training.
- Not recommended for safety-critical systems without validation.

---

## üöÄ Intended Use

This repository is intended for:

- Educational purposes
- Deep learning practice
- Internship or academic evaluation
- Demonstrating knowledge of training internals

Not intended for direct production deployment without further extension.